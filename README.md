<p align="center">
  <img src="assets/EmoDubber_Logo.png" width="30%" />
</p>
<div align="center">
  <h3 class="papername"> 
    EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing </h3>
</div>


[![python](https://img.shields.io/badge/Python-3.10-blue)](https://github.com/GalaxyCong/DubFlow)
[![arXiv](https://img.shields.io/badge/arXiv-2406.06937-b31b1b.svg?logo=arXiv)](https://arxiv.org/pdf/2412.08988)
[![code](https://img.shields.io/badge/Github-Code-keygen.svg?logo=github)](https://github.com/GalaxyCong/DubFlow)
[![demo](https://img.shields.io/badge/GitHub-Demo%20page-orange.svg)](https://galaxycong.github.io/EmoDub/)



# 

# ðŸ—’ TODO List
- [âœ“] Release EmoDubber's training and inference code (Basic Fuction). (Fixed on 5/27/2025)
- [âœ“] Upload pre-processed dataset features to Baidu Cloud and Google Cloud. (Done 5/27/2025)
- [âœ“] Release model checkpoint (Basic Fuction) to inference waveform. (Before 6/1/2025)
- [-] Release EmoDubber's emotion controlling code (Emotion Fuction). 
- [-] Provide metrics testing scripts (LSE-C, LSE-D, SECS, WER, MCD). 


![Illustration](./assets/model_fig_1.jpeg)

# Environment

1. Clone this repository:
```bash
git clone https://github.com/GalaxyCong/EmoDubber.git
cd EmoDubber
```
2. Create an environment
```bash
conda create -n emodub python=3.10 -y
conda activate emodub
```
3. Install python requirements: 
```bash
pip install -r requirements.txt
```
4. Install [monotonic_align](https://github.com/resemble-ai/monotonic_align)
```bash
pip install git+https://github.com/resemble-ai/monotonic_align.git
```
5. Last step. Download [trainer.py](https://drive.google.com/file/d/1pfMlL22zz8xDcLYtN_cUTp8lMUquN4zD/view?usp=sharing) to replace your ```anaconda3/envs/emodub/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py```;  
Download [checkpoint_connector.py](https://drive.google.com/file/d/1KkLOEsmG82znIHOIN0iHEfqR6ZyLY8yb/view?usp=sharing) to replace your ```anaconda3/envs/emodub/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py```

(Note: Step5 is to prevent the error of Missing key(s) in state_dict (TTS_model.ckpt >> EmoDubber_all). I avoid this problem by setting "strict=False" in torch lightning.)


# Prepare Data Feature

When performing training, both Raw Audio and Prosessed Features need to be downloaded. 
When inferencing, only Prosessed Features are needed. 

## Chem 

- Chem 16KHz Raw Audio: [Google Drive](https://drive.google.com/file/d/1gPwUePAkJFZJ5Xm7YrOuxCQN5NIbG5iF/view?usp=sharing) || [Baidu Drive](https://pan.baidu.com/s/1RD7gzSs3XeimTJVJOkkjxA)(erap)


- Chem Prosessed Feature: [Google Drive](https://drive.google.com/file/d/1xwx5cD8t24JPu3_t-Rqwreg0iH6rj5a1/view?usp=sharing) || [Baidu Drive](https://pan.baidu.com/s/198s3QM_Fi47kCPe6umzxyA)(nriv)


## GRID

- GRID 16KHz Raw Audio: [Google Drive](https://drive.google.com/file/d/1V0wyhmEKwB4N58w0GO4AX9WSLFLn86LE/view?usp=sharing) || [Baidu Drive](https://pan.baidu.com/s/174hmeiareDW51IG_a58AFg)(xikd)

- GRID Prosessed Feature: [Google Drive](https://drive.google.com/file/d/13h_htzQwZfZDEtfQ6BF-Y-99uAiSxyEA/view?usp=sharing) || [Baidu Drive](https://pan.baidu.com/s/1vpoaiXfYnu5RPmu3OKmkew)(cbdy)



# Train Your Own Model

1. Ensure input path is correct (see ```configs/data/Chem_dataset.yaml``` or ```configs/data/GRID_dataset```).
2. Download [TTS_model.ckpt](https://drive.google.com/file/d/14NbtYF07fKxw13MQJfhtD2NwizRpJFY5/view?usp=sharing) (pretraining on LibriTTS-clean-100 dataset) and save it in ```Pretrained_TTSmodel``` folder. 
3. Finally, please stay in root directory, and run directly: 
```bash
python EmoDubber_Networks/Train_EmoDubber_Chem16K.py
```
or
```bash
python EmoDubber_Networks/Train_EmoDubber_GRID16K.py
```


# Our Checkpoints

We provide EmoDubber's checkpoints (Basic Fuction). Moreover, we also provide the audio generated by these checkpoints. They were used to compare with other SOTA dubbing baseline in main setting (Setting1 & Setting 2), .ie, without emotion control. It will hopefully facilitate future comparisons. 

The links are given below:

## Checkpoints on Chem dataset

- Checkpoint: [Google Drive](https://drive.google.com/file/d/1CqTZU98xmHMy9C9Q0gFHvGyaZjUifizO/view?usp=sharing) or [Baidu Drive](https://pan.baidu.com/s/1M6anLZ7DUMRvVGapwJ2aaQ)(sxus)

- Generated Result: [Google Drive](https://drive.google.com/file/d/1NQn2T5aLghUwknnQiJTp_5wecsnlkrOr/view?usp=sharing) or [Baidu Drive](https://pan.baidu.com/s/16cwOmLFr3FCH4fbuWoFBQA)(heu2)



## Checkpoints on GRID dataset

- Checkpoint: [Google Drive](https://drive.google.com/file/d/1RRf--kPzldhHro6jbauh2sP29iphIh-h/view?usp=sharing) or [Baidu Drive](https://pan.baidu.com/s/1_7Orv8ccIxa_WlZD--GR6Q)(hv9t)

- Generated Result: [Google Drive](https://drive.google.com/file/d/1zsGYLDA25AfSTPEJuIn9edp3jjbdFXTn/view?usp=sharing) or [Baidu Drive](https://pan.baidu.com/s/1gB-riJ1QYwpeCI7KdGTNCA)(2ibw)



# Inference 

1. Download EmoDubber's [16k Hz Vocoder](https://drive.google.com/file/d/1XXCSYbIEjePcWT8jNwSWeLlIQf0M-2pp/view?usp=sharing) and save it to ```Vocoder_16KHz``` folder. (--vocoder_checkpoint_path "-path")


2. Run script for inference (stay in root): 
```bash
python EmoDubber_Networks/Inference_Chem_Unbatch_New.py --checkpoint_path "-path" --vocoder_checkpoint_path "-path" --Val_list "-path" --Silent_Lip "-path" --Silent_Face "-path" --Refence_audio "-path"
```


```bash
python EmoDubber_Networks/Inference_GRID_Unbatch_New.py --checkpoint_path "-path" --vocoder_checkpoint_path "-path" --Val_list "-path" --Silent_Lip "-path" --Silent_Face "-path" --Refence_audio "-path"
```



# Emotion Controlling  
Under construction

## Training emotional expert classifier

We provide all checkpoints. Below is the checkpoint of our emotional expert classifier. 

Five types of emotions (recommend): https://drive.google.com/drive/folders/1vSVTAkZsoinSlYgeVCvBXs5V2k-gIurV?usp=sharing


Seven types of emotions (recommend): https://drive.google.com/drive/folders/1h0Y1TChA9vgX3_6u5GUrJK69n0rgKTRU?usp=sharing


Seven types of emotions with unemotional data augmentation: https://drive.google.com/drive/folders/1DuhQYe5FuowHBRMOFRhthrvPJBlfK_5E?usp=sharing


## Inference 

Under construction


# License

Code: MIT License


# Citing

If you find this helps your research, please consider citing:
```BibTeX
@article{cong2024emodubber,
  title={EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing},
  author={Cong, Gaoxiang and Pan, Jiadong and Li, Liang and Qi, Yuankai and Peng, Yuxin and Hengel, Anton van den and Yang, Jian and Huang, Qingming},
  journal={arXiv preprint arXiv:2412.08988},
  year={2024}
}
```

# Contact

My email is gaoxiang.cong@vipl.ict.ac.cn


Any discussions and suggestions are welcome!


